{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Create the XOR gate's truth table dataset.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#  XOR gate's truth table\n",
        "data = {'input1': [0, 0, 1, 1], 'input2': [0, 1, 0, 1], 'output': [0, 1, 1, 0]}\n",
        "xor_df = pd.DataFrame(data)\n",
        "\n",
        "print(xor_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAbA4Ko-1if_",
        "outputId": "a7e7a615-8ce2-4038-9b7f-27355747470f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   input1  input2  output\n",
            "0       0       0       0\n",
            "1       0       1       1\n",
            "2       1       0       1\n",
            "3       1       1       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the perceptron model and train it using the XOR dataset using MCP (McCulloch Pitts) Neuron.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the activation function (step function)\n",
        "def activation_function(x):\n",
        "  return 1 if x >= 0 else 0\n",
        "\n",
        "# Define the perceptron model\n",
        "class Perceptron:\n",
        "  def __init__(self, input_size, learning_rate=0.1):\n",
        "    self.weights = np.random.rand(input_size)\n",
        "    self.bias = np.random.rand()\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def predict(self, inputs):\n",
        "    linear_output = np.dot(inputs, self.weights) + self.bias\n",
        "    return activation_function(linear_output)\n",
        "\n",
        "  def train(self, training_inputs, labels, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      for inputs, label in zip(training_inputs, labels):\n",
        "        prediction = self.predict(inputs)\n",
        "        error = label - prediction\n",
        "        self.weights += self.learning_rate * error * inputs\n",
        "        self.bias += self.learning_rate * error\n",
        "\n",
        "# Prepare the XOR dataset\n",
        "inputs = xor_df[['input1', 'input2']].values\n",
        "labels = xor_df['output'].values\n",
        "\n",
        "# Create and train the perceptron\n",
        "perceptron = Perceptron(input_size=2, learning_rate=0.1)\n",
        "perceptron.train(inputs, labels, epochs=100)\n",
        "\n",
        "# Test the perceptron\n",
        "for input_pair in inputs:\n",
        "  prediction = perceptron.predict(input_pair)\n",
        "  print(f\"Input: {input_pair}, Prediction: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdPXI1kZ3dJc",
        "outputId": "db825257-ac40-4d09-bf82-69a4bc8179b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Prediction: 1\n",
            "Input: [0 1], Prediction: 1\n",
            "Input: [1 0], Prediction: 0\n",
            "Input: [1 1], Prediction: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observe and discuss the perceptron's performance in this scenario.**\n",
        "\n",
        "The output is not correct . All of the predicted values are wrong.This is beacuse the single-layered perceptron can only calssify linerly seperable data and XOR is not lineraly sepreable. Here eveen if some values did get right , the values will not be accurate always."
      ],
      "metadata": {
        "id": "miFjvEgI28lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement XOR using Multi-Layer Perceptron.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "# Define the neural network class\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Initialize weights and biases randomly\n",
        "    self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "    self.bias_hidden = np.random.randn(self.hidden_size)\n",
        "    self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "    self.bias_output = np.random.randn(self.output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Calculate the hidden layer output\n",
        "    self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
        "    self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "    # Calculate the output layer output\n",
        "    self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "    self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "    return self.output_layer_output\n",
        "\n",
        "  def backward(self, inputs, labels, learning_rate):\n",
        "    # Calculate the error at the output layer\n",
        "    output_error = labels - self.output_layer_output\n",
        "    output_delta = output_error * sigmoid_derivative(self.output_layer_output)\n",
        "\n",
        "    # Calculate the error at the hidden layer\n",
        "    hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "    hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta) * learning_rate\n",
        "    self.bias_output += np.sum(output_delta, axis=0) * learning_rate\n",
        "    self.weights_input_hidden += inputs.T.dot(hidden_delta) * learning_rate\n",
        "    self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate\n",
        "\n",
        "  def train(self, inputs, labels, epochs, learning_rate):\n",
        "    for epoch in range(epochs):\n",
        "      output = self.forward(inputs)\n",
        "      self.backward(inputs, labels, learning_rate)\n",
        "\n",
        "# Prepare the XOR dataset\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "labels = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create and train the neural network\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(inputs, labels, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# Test the neural network\n",
        "for input_pair in inputs:\n",
        "  prediction = nn.forward(input_pair)\n",
        "  print(f\"Input: {input_pair}, Prediction: {round(prediction[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkRmOZl93fqc",
        "outputId": "87d419b4-5db8-49c6-b661-670c05aafcbf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Prediction: 0\n",
            "Input: [0 1], Prediction: 1\n",
            "Input: [1 0], Prediction: 1\n",
            "Input: [1 1], Prediction: 0\n"
          ]
        }
      ]
    }
  ]
}